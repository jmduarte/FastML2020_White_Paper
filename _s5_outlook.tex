% As we discussed, there are lots of works on NN for science.  We have ways of doing X, Y, and Z.  And it is clear that, as we go forward, high throughput and low latency machine learning techniques will be critical for future Science deployments.  However, we still need innovation on A, B, and C.

This report has laid out exciting applications of fast ML to enable scientific discovery across a number of domains.  
This is a rapidly developing area with many exciting new studies and results  appearing often.  
However, this is a relatively young area rich with potential and a number of open challenges across a number of fields.  
Beyond what has been laid out in the report, we hope that the discussion of scientific use-cases and their overlaps will provide readers with inspiration to entertain and pursue additional applications.  

In Section~\ref{sec:technolog_sota}, we provided an overview of techniques for developing powerful ML algorithms that need be operated in high throughput and low latency environments.  
This includes both systematic design and training as well as efficient deployment and implementation of those ML models.  
Implementation in hardware is discussed under two main categories---current conventional CMOS and more speculative beyond CMOS technologies.  In the conventional CMOS case,  in light of the end of Moore's Law, the recent emphasis has been focused on advanced hardware architectures designed for ML.  
We gave an overview of popular and emerging hardware architectures and their strengths and shortcomings. 
A key area of importance for the multitude of hardware is their codesign of a given ML algorithm for a specific hardware including the architecture and programmability of that algorithm.  
An example of a particularly relevant and important hardware platform is for FPGAs and that is the use-case discussed in Section~\ref{sec:codesign}.  
Finally we concluded with an overview of beyond CMOS technologies which offer exciting and ultra-efficient technologies on which we can implement ML models.  
While these technologies are speculative, they offer potential orders of magnitude improvement over conventional technologies.

Both ML training and deployment techniques and computer architectures are extremely rapidly moving fields with new works appearing a pace difficult to keep up with, even for this report.  
While new methods are being introduced continuously in both spaces, it is particularly important to understand the codesign of new algorithms for different hardware and the ease of use of the tool flows for deploying those algorithms.  
Innovations here will allow rapid and broad adoption of powerful new ML hardware.  
In the case of beyond CMOS technologies, these practical considerations are important as well as considering the maturity of the technology, integration into computing architectures, and how to program such devices.  

We look forward to revisiting these topics in the near future to see how quickly advances may come in applications, ML techniques, and hardware platforms---and most importantly their confluence to enable paradigm-shifting breakthroughs in science.

