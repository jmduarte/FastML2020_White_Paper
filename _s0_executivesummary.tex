Machine learning (ML) is making a huge impact on our society and daily lives through advancements in computer vision, natural language processing, and autonomous vehicles, among others.  
ML is also powering scientific advances which can lead to future paradigm shifts in a broad range of domains, including particle physics, plasma physics, astronomy, neuroscience, chemistry, material science, and biomedical engineering.  
Scientific discoveries come from groundbreaking ideas and the capability to validate those ideas by testing nature at new scales---finer and more precise temporal and spatial resolution.  
This is leading to an explosion of data that must be interpreted, and ML is proving a powerful approach. 
The more efficiently we can test our hypotheses, the faster we can achieve discovery.  
To fully unleash the power of ML and accelerate discoveries, it is necessary to embed it into our scientific process, into our instruments and detectors.

It is in this spirit that the Fast Machine Learning for Science community\footnote{\url{fastmachinelearning.org}} has been built.  
Two workshops have also been organized through this growing community and is the source for this report.  
The community brings together an extremely wide-ranging group of domain experts who would rarely interact as a whole. 
One of the underlying benefits of ML is the portability and general applicability of the techniques that can enable experts from seemingly unrelated domains to find a common language. 
Scientists and engineers from particle physicists to networking experts and biomedical engineers are represented and can interact with experts in fundamental ML techniques and compute systems architects.  

This report aims to summarize the progress in the community to understand how our scientific challenges overlap and where there are potential commonalities in data representations, ML approaches, and technology, including hardware and software platforms. 
Therefore, \textbf{the content of the report includes the following: descriptions of a number of different scientific domains including existing work and applications for embedded ML; potential overlaps across scientific domains in data representation or system constraints; and an overview of state-of-the-art techniques for efficient machine learning and compute platforms, both cutting-edge and speculative technologies}.  

Necessarily, such a broad scope of topics \textit{cannot} be comprehensive. 
For the scientific domains, we note that the contributions are \textit{examples} of how ML methods are currently being or planned to be deployed.  
We hope that giving a glimpse into specific applications will inspire readers to find more novel use-cases and potential overlaps. 
The summaries of state-of-the-art techniques we provide relate to rapidly developing fields and, as such, may become out of date relatively quickly.  
The goal is to give non-experts an overview and taxonomy of the different techniques and a starting point for further investigation.
To be succinct, we rely heavily on providing references to studies and other overviews while describing most modern methods.

We hope the reader finds this report both instructive and motivational.  
Feedback and input to this report, and to the larger community, is welcome and appreciated.  

\begin{flushright} 
\textit{
Sincerely, \\The Editors
}
\end{flushright}